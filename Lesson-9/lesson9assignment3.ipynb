{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 regularizetion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reload data we saved in assigment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (8947, 28, 28) (8947,)\n",
      "test set (8691, 28, 28) (8691,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_root = '/home/student/project/project-01/nlp_bots/hw-xujing'\n",
    "pickle_file = os.path.join(data_root, 'notMNIST_clean.pickle')\n",
    "pickle_file1 = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del(save)\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (8947, 784) (8947, 10)\n",
      "Test set (8691, 784) (8691, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "           / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune L2 regularizition for both logisitc and neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-27d340dfe82a>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input data, training data using placeholder\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #variables\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #training computation\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    #original loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    #Loss function using L2 regularizition\n",
    "    regularizer = beta_regul*tf.nn.l2_loss(weights)\n",
    "    loss = tf.reduce_mean(loss + regularizer)\n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #predictions for the training, validation and test\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) +biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 20.747681\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 500: 3.141049\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 1000: 1.726962\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1500: 1.194721\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 2000: 0.899220\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2500: 0.736119\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 3000: 1.093994\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 80.4%\n",
      "Test accuracy: 87.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "##3001*128 总共的训练数据   ## 之前是801*10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        ##分母减去batch_size是防止当batch_size不能被训练数据量整除时，offset:(offset + batch_size)超出数组界限\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the different value of L2 regulariition parameters in ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        #f.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Test accuracy by regularization (logistic)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural network with L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input data training data use a placeholder that will be fed \n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #Training computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    \n",
    "    #original loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    #L2 regularization\n",
    "    regularizer = beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    loss = tf.reduce_mean(loss + regularizer)\n",
    "    \n",
    "    #optimizer \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #predictions for the training, validation, test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) +biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) +biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 645.278381\n",
      "Minibatch accracy: 9.4%\n",
      "Validation accuracy: 28.0%\n",
      "Minibatch loss at step 500: 199.720947\n",
      "Minibatch accracy: 76.6%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 1000: 116.298141\n",
      "Minibatch accracy: 81.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 1500: 68.671471\n",
      "Minibatch accracy: 84.4%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2000: 41.250896\n",
      "Minibatch accracy: 86.7%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2500: 24.994606\n",
      "Minibatch accracy: 91.4%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3000: 15.610190\n",
      "Minibatch accracy: 83.6%\n",
      "Validation accuracy: 86.4%\n",
      "Test accurracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001 # data quantity 3001*128\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        #pick an offset within the training data, which has been randomized\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        #generate a minibatch\n",
    "        batch_data = train_dataset[offset:(offset+ batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        #prepare a dictionary telling the session where to feed the minibatch\n",
    "        # the key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        #and the value is the numpy array to feed to it\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accurracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            #pick an offset within the training data, which has been randomized\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            #generate a minibatch\n",
    "            batch_data = train_dataset[offset:(offset+ batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset+ batch_size), :]\n",
    "            #prepare a dictionary telling teh session where to feed teh minibatch\n",
    "            #the key of the dictionary is the placeholder node of the graph to be fed\n",
    "            #the value is the numpy array to feed to it\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Test accuracy by regularization (1-layer neural network)')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer neural network)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "over fitting case too few batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input training data with placeholder\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes])) #28*28 * 1024\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes])) #1024\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels])) #1024 * num_labels\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels])) # num_labels\n",
    "    \n",
    "    # training computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) +biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    \n",
    "    #original loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    # regularizer = beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    # loss = tf.reduce_mean(loss + regularizer)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #predictions for training validation, test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) +biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "minibatch loss at step 0: 332.258667\n",
      "minibatch accuracy: 11.7%\n",
      "validation accuracy: 25.5%\n",
      "minibatch loss at step 10: 60.674568\n",
      "minibatch accuracy: 12.5%\n",
      "validation accuracy: 70.5%\n",
      "minibatch loss at step 20: 1.532516\n",
      "minibatch accuracy: 8.6%\n",
      "validation accuracy: 71.2%\n",
      "minibatch loss at step 30: 0.000000\n",
      "minibatch accuracy: 11.7%\n",
      "validation accuracy: 71.2%\n",
      "minibatch loss at step 40: 0.941779\n",
      "minibatch accuracy: 12.5%\n",
      "validation accuracy: 71.3%\n",
      "minibatch loss at step 50: 0.794447\n",
      "minibatch accuracy: 8.6%\n",
      "validation accuracy: 71.2%\n",
      "minibatch loss at step 60: 0.000000\n",
      "minibatch accuracy: 11.7%\n",
      "validation accuracy: 71.2%\n",
      "minibatch loss at step 70: 0.715169\n",
      "minibatch accuracy: 12.5%\n",
      "validation accuracy: 71.3%\n",
      "minibatch loss at step 80: 0.139666\n",
      "minibatch accuracy: 8.6%\n",
      "validation accuracy: 71.2%\n",
      "minibatch loss at step 90: 0.000000\n",
      "minibatch accuracy: 11.7%\n",
      "validation accuracy: 71.2%\n",
      "minibatch loss at step 100: 0.000270\n",
      "minibatch accuracy: 12.5%\n",
      "validation accuracy: 71.2%\n",
      "test accuracy: 78.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] -batch_size) # small data training over again\n",
    "        #generate a minibatch\n",
    "        batch_data = train_dataset[offset:(offset+ batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        # minibatch dictionary (placeholder node of the graph tobe fed) : numpy array to feed to \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, prediction = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 10 ==0):\n",
    "            print('minibatch loss at step %d: %f' % (step, l))\n",
    "            print('minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probelm 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "introduce dorpout to hidden layer during training in overfitting case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic behind \"dropout\" --- \"dropout\" means we randomly deactivate a certain number of neurons in the network during training, so it is like we are drawing a random \"sample\" from different network structures and by \"averaging\" the predictions produced by those different networks, we reduce the error related to overfitting (i.e., the model becomes more generalizable on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32) ##addL2\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))##隐藏层1024个节点\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.##设置神经网络tf.nn.relu()里的参数相当于逻辑回归的函数logits\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5) #add drop\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \n",
    "    #+beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))##add L2\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1) \n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 393.787567\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 36.5%\n",
      "Minibatch loss at step 10: 54.292122\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 20: 14.013303\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 30: 5.676493\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 40: 2.494643\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 50: 3.716099\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 60: 3.858839\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 70: 0.717485\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 90: 1.094872\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 100: 3.934409\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.5%\n",
      "Test accuracy: 83.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        ##这里把相当于固定很小一部分的训练数据，然后一直训练这部分数据，所以可得到过拟合现象\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one is to use learning rate decay:\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "tf.train.exponential_decay(\n",
    "learning_rate,\n",
    "global_step,\n",
    "decay_steps,\n",
    "decay_rate,\n",
    "staircase=False,\n",
    "name=None,)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.exponential_decay??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2层神经网络\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    #第一层输入节点是28*28级原始数据的维度大小，本层节点是个数是num_hidden_nodes1\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes1],\n",
    "                                               stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    \n",
    "    #第二层的输入节点是第一层节点个数，本层节点个数是num_hidden_nodes2,stddev是指定生成数据的标准差\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    #最后一层的输入节点个数是第二层节点个数，本层节点个数是要分类的类别个数。\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.267127\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 28.6%\n",
      "Minibatch loss at step 500: 1.135358\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1000: 0.772177\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1500: 0.681346\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2000: 0.559601\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2500: 0.468889\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 3000: 0.757255\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3500: 0.536709\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4000: 0.542546\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 4500: 0.427785\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 5000: 0.465918\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 5500: 0.456172\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 6000: 0.508089\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6500: 0.496241\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 7000: 0.428600\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 7500: 0.512889\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 8000: 0.508410\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 8500: 0.392578\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9000: 0.442315\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "Test accuracy: 95.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#三层神经网络\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable( tf.truncated_normal([image_size * image_size, num_hidden_nodes1], \n",
    "                                                stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, \n",
    "                                                num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(tf.truncated_normal([num_hidden_nodes3, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "    logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)+ tf.nn.l2_loss(weights4))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.466478\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 33.1%\n",
      "Minibatch loss at step 500: 1.262836\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1000: 0.833835\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1500: 0.735379\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2000: 0.605900\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 2500: 0.468212\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 3000: 0.730717\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 3500: 0.567295\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4000: 0.690929\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 4500: 0.404070\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 5000: 0.455640\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 5500: 0.427458\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 6000: 0.496910\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 6500: 0.514884\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 7000: 0.426334\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 7500: 0.476578\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8000: 0.550975\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8500: 0.351991\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 9000: 0.399790\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 9500: 0.444289\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 10000: 0.308027\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 10500: 0.464527\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 11000: 0.448899\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 11500: 0.477125\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 12000: 0.374366\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12500: 0.473902\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 13000: 0.530105\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 13500: 0.366133\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 14000: 0.288466\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 14500: 0.335752\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 15000: 0.476750\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 15500: 0.363390\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 16000: 0.442152\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 16500: 0.476609\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17000: 0.361271\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17500: 0.273724\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 18000: 0.392514\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Test accuracy: 95.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###3层神经网络+随机失活(dropout)\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable( tf.truncated_normal([image_size * image_size, num_hidden_nodes1], \n",
    "                                                stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, \n",
    "                                                num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(tf.truncated_normal([num_hidden_nodes3, num_labels], \n",
    "                                               stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "    drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "    drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "    logits = tf.matmul(drop3, weights4) + biases4\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) #+ \\\n",
    "     # beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)+tf.nn.l2_loss(weights4))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
