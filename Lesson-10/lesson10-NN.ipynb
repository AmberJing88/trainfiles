{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lesson 10 inclass code practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a ancester class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node: \n",
    "##forward: Function, how to calculate the inputs\n",
    "##backward: Function, how to get the gradients of the nodes, when backpropagation\n",
    "##gradients: mapper, the gradient of this node to its inputs node\n",
    "##inputs: list, the inputs nodes of this node\n",
    "##outputs: list, the outputs nodes of this node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Each node will have these attributions and methods\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs=[]):\n",
    "        \"\"\"\n",
    "        if the node is anoprator of ax+b, its input will be x, and output will its successors, so the \n",
    "        inputs and outputs will be the connected nodes to the node\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        self.value = None\n",
    "        self.gradients = {}\n",
    "        \n",
    "        for node in self.inputs:\n",
    "            node.outputs.append(self) #build connected relationship\n",
    "            \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        compute the output value based on the input nodes\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        compute the gradients of current note based on the input nodes\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    '''\n",
    "    if the node is the operator of \"ax + b\"; see a , b , x both as a node object\n",
    "    So this Input object is for feedding data to the layer.\n",
    "    In order to make graph simple ,and casue it's fully connected network ,we can see one layer as as Node\n",
    "    '''\n",
    "    def __init__(self, name, inputs=[]):\n",
    "        self.name = name\n",
    "        super().__init__(inputs)\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        if value:\n",
    "            self.value = value\n",
    "            \n",
    "    def backward(self):\n",
    "        self.gradients = {}\n",
    "         #从输出节点中获取loss函数对该节点的偏导，而用该值乘该节点对input参数的导数即得到loss对input的导数\n",
    "        #由此可见该节点可以得到loss对input的导数，这也是为什么该节点直接从output节点中获取loss函数对该节点的偏导\n",
    "        for node in self.outputs:\n",
    "            grad_cost = node.gradients[self]\n",
    "            self.gradients[self] = grad_cost\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return \"Input Node: {}\".format(self.name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        '''\n",
    "        nodes, weigths,bias is Input node object\n",
    "        '''\n",
    "        self.w_node = weights\n",
    "        self.x_node = nodes\n",
    "        self.b_node = bias\n",
    "        super().__init__(inputs=[nodes, weights, bias])\n",
    "        \n",
    "    def forward(self):\n",
    "        '''\n",
    "        compute the a*x + b, In order to make it available for matric input ,\n",
    "        we use numpy  to caculate    \n",
    "        '''\n",
    "        #print(self.x_node.value.shape, self.w_node.value.shape,'33333')\n",
    "        self.value = np.dot(self.x_node.value, self.w_node.value.T) + self.b_node.value\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        linear function get gradients of nodes\n",
    "        \"\"\"\n",
    "        for node in self.outputs:\n",
    "            grad_cost = node.gradients[self]\n",
    "            self.gradients[self] = grad_cost\n",
    "            self.gradients[self.w_node] = np.dot(grad_cost.T, self.x_node.value)\n",
    "            self.gradients[self.x_node] = np.dot(grad_cost, self.w_node.value)\n",
    "            self.gradients[self.b_node] = grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        self.x_node = node\n",
    "        super().__init__([node])\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1. / (1 + np.exp(-1 * x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = self._sigmoid(self.x_node.value)\n",
    "        \n",
    "    def backward(self):\n",
    "        y = self.value\n",
    "        self.partial = y * (1 - y)\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost\n",
    "            #不管是矩阵输入还是单个输入，self.partial都是一个一维常数，所以可以直接相乘\n",
    "            self.gradients[self.x_node] = grad_cost * self.partial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y_true, y_hat):\n",
    "        self.y_true_node = y_true\n",
    "        self.y_hat_node = y_hat\n",
    "        super().__init__(inputs=[y_true, y_hat])\n",
    "        \n",
    "    def forward(self):\n",
    "        y_true_flatten = self.y_true_node.value.reshape(-1, 1)\n",
    "        y_hat_flatten = self.y_hat_node.value.reshape(-1, 1)\n",
    "        self.diff = y_true_flatten - y_hat_flatten\n",
    "        self.value = np.mean(self.diff ** 2)\n",
    "        \n",
    "    def backward(self):\n",
    "        n = self.y_hat_node.value.shape[0]\n",
    "        self.gradients[self.y_true_node] = (2 / n) * self.diff\n",
    "        self.gradients[self.y_hat_node] = (-2 / n) * self.diff\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_one_batch(topological_sorted_graph):\n",
    "    # graph 是经过拓扑排序之后的 一个list\n",
    "    for node in topological_sorted_graph:\n",
    "        node.forward()\n",
    "    for node in topological_sorted_graph[::-1]:\n",
    "        node.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0.9999991684719722\n",
      "0.4899988358614525\n"
     ]
    }
   ],
   "source": [
    "#单节点传播测试\n",
    "#当输入数据变成矩阵时，相当于单节点就变成了layer\n",
    "step1 = 3 * 4 + 2 \n",
    "print(step1)\n",
    "step2 = 1. / (1 + np.exp(-1* step1))\n",
    "print(step2)\n",
    "loss = (step2 - 0.3) ** 2\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Input('x')\n",
    "y = Input('y')\n",
    "w = Input('w')\n",
    "b = Input('b')\n",
    "linear_output = Linear(x, w, b)\n",
    "y_hat = Sigmoid(linear_output)\n",
    "loss = MSE(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.value = np.array([3])\n",
    "y.value = np.array([0.3])\n",
    "w.value = np.array([4])\n",
    "b.value = np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_output.forward()\n",
    "y_hat.forward()\n",
    "loss.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "y_hat.backward()\n",
    "linear_output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14] [0.99999917] 0.4899988358614525\n"
     ]
    }
   ],
   "source": [
    "print(linear_output.value, y_hat.value, loss.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topological_sort(data_with_value):\n",
    "    inputs_nodes = [n for n in data_with_value.keys()]\n",
    "    nodes = inputs_nodes.copy()\n",
    "    G= {}\n",
    "    \n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0) #广度优先搜索找出每个节点的in节点和out节点\n",
    "        if n not in G:\n",
    "            G[n] = {'in' : set(), 'out' : set()}\n",
    "            for m in n.outputs:\n",
    "                if m not in G:\n",
    "                    G[m] = {'in' : set(), 'out' : set()}\n",
    "                G[n]['out'].add(m)\n",
    "                G[m]['in'].add(n)\n",
    "                nodes.append(m)\n",
    "                \n",
    "    L = []\n",
    "    S = set(inputs_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "        L.append(n)\n",
    "        for  m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) ==0:\n",
    "                S.add(m)\n",
    "    return L     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topological_sort(data_with_value):\n",
    "    input_nodes = [n for n in data_with_value.keys()]\n",
    "    \n",
    "    nodes = input_nodes.copy()\n",
    "    G = {}\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0) #广度优先搜索找出每个节点的in节点和out节点\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "#         if isinstance(n, Input):\n",
    "#             n.value = feed_dict[n] #feed orgin value, it's ok do it somewhere else        \n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_update(trainable_nodes, learning_rate=1e-2):\n",
    "    '''\n",
    "    因为我们要优化的是w和b这些参数矩阵，所以trainable_nodes就应该是这些参数代表的节点\n",
    "    '''\n",
    "    for t in trainable_nodes:\n",
    "        step = -1 * learning_rate * t.gradients[t]\n",
    "        t.value = t.value + step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_data(feed_dict):\n",
    "    for key in feed_dict.keys():\n",
    "        key.value = feed_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 1234\n",
      "Epoch: 1, loss = 151.259\n",
      "Epoch: 101, loss = 7.181\n",
      "Epoch: 201, loss = 3.516\n",
      "Epoch: 301, loss = 3.202\n",
      "Epoch: 401, loss = 2.497\n",
      "Epoch: 501, loss = 1.934\n",
      "Epoch: 601, loss = 1.616\n",
      "Epoch: 701, loss = 1.450\n",
      "Epoch: 801, loss = 1.301\n",
      "Epoch: 901, loss = 0.991\n",
      "Epoch: 1001, loss = 0.984\n",
      "Epoch: 1101, loss = 1.080\n",
      "Epoch: 1201, loss = 0.867\n",
      "Epoch: 1301, loss = 0.738\n",
      "Epoch: 1401, loss = 0.887\n",
      "Epoch: 1501, loss = 0.852\n",
      "Epoch: 1601, loss = 0.749\n",
      "Epoch: 1701, loss = 0.765\n",
      "Epoch: 1801, loss = 0.562\n",
      "Epoch: 1901, loss = 0.619\n",
      "Epoch: 2001, loss = 0.662\n",
      "Epoch: 2101, loss = 0.574\n",
      "Epoch: 2201, loss = 0.534\n",
      "Epoch: 2301, loss = 0.503\n",
      "Epoch: 2401, loss = 0.498\n",
      "Epoch: 2501, loss = 0.409\n",
      "Epoch: 2601, loss = 0.439\n",
      "Epoch: 2701, loss = 0.529\n",
      "Epoch: 2801, loss = 0.386\n",
      "Epoch: 2901, loss = 0.439\n",
      "Epoch: 3001, loss = 0.405\n",
      "Epoch: 3101, loss = 0.316\n",
      "Epoch: 3201, loss = 0.296\n",
      "Epoch: 3301, loss = 0.316\n",
      "Epoch: 3401, loss = 0.334\n",
      "Epoch: 3501, loss = 0.364\n",
      "Epoch: 3601, loss = 0.327\n",
      "Epoch: 3701, loss = 0.298\n",
      "Epoch: 3801, loss = 0.262\n",
      "Epoch: 3901, loss = 0.279\n",
      "Epoch: 4001, loss = 0.260\n",
      "Epoch: 4101, loss = 0.325\n",
      "Epoch: 4201, loss = 0.207\n",
      "Epoch: 4301, loss = 0.201\n",
      "Epoch: 4401, loss = 0.196\n",
      "Epoch: 4501, loss = 0.219\n",
      "Epoch: 4601, loss = 0.226\n",
      "Epoch: 4701, loss = 0.224\n",
      "Epoch: 4801, loss = 0.198\n",
      "Epoch: 4901, loss = 0.184\n"
     ]
    }
   ],
   "source": [
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden1 = 64 #隐藏层是64个节点\n",
    "n_hidden2 = 64 #隐藏层是64个节点\n",
    "n_out = 1\n",
    "\n",
    "W1_, b1_ = np.random.randn(n_hidden1, n_features), np.zeros(n_hidden1)\n",
    "W2_, b2_ = np.random.randn(n_hidden2, n_hidden1), np.zeros(n_hidden2)\n",
    "W3_, b3_ = np.random.randn(1, n_hidden2), np.zeros(1)\n",
    "\n",
    "# Input data. For the training data\n",
    "X, y = Input(name='X'), Input(name='y')\n",
    "W1, b1 = Input(name='W1'), Input(name='b1')\n",
    "W2, b2 = Input(name='W2'), Input(name='b2')\n",
    "W3, b3 = Input(name='W3'), Input(name='b3')\n",
    "\n",
    "#network connection\n",
    "linear_output1 = Linear(X, W1, b1)\n",
    "sigmoid_output1 = Sigmoid(linear_output1)\n",
    "linear_output2 = Linear(sigmoid_output1, W2, b2)\n",
    "sigmoid_output2= Sigmoid(linear_output2)\n",
    "yhat = Linear(sigmoid_output2, W3, b3)\n",
    "loss = MSE(y, yhat)\n",
    "\n",
    "# get topological sort, use this order to  compute Back propagation\n",
    "input_node_with_value = {X:X_, y:y_, W1: W1_, W2: W2_, W3: W3_,b1: b1_, b2: b2_,b3: b3_}\n",
    "graph = topological_sort(input_node_with_value)\n",
    "\n",
    "#feed initial weight and bias of each layer\n",
    "feed_dict = {W1 : W1_, W2:W2_, W3:W3_, b1:b1_, b2:b2_, b3:b3_}\n",
    "feed_data(feed_dict)\n",
    "print(len(W1.value), '1234')\n",
    "\n",
    "losses =[]\n",
    "epochs = 5000\n",
    "batch_size = 96\n",
    "step_per_epoch = X_.shape[0] // batch_size\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for _step in range(step_per_epoch):\n",
    "        #random choose data\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "        feed_dict = {X:X_batch, y:y_batch}\n",
    "        feed_data(feed_dict)\n",
    "        #print(len(X.value), '111')\n",
    "        \n",
    "        training_one_batch(graph) #训练一次\n",
    "        sgd_update(trainable_nodes = [W1, W2, W3, b1, b2, b3], learning_rate=learning_rate)\n",
    "        \n",
    "        loss += graph[-1].value\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print('Epoch: {}, loss = {:.3f}'.format(i+1, loss / step_per_epoch))\n",
    "        losses.append(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fb4aadf828>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQf4/fkNiad5evwYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fb4aa65358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 64)\n",
      "(96, 64)\n",
      "(96, 64)\n"
     ]
    }
   ],
   "source": [
    "print(linear_output1.value.shape)\n",
    "print(sigmoid_output1.value.shape)\n",
    "print(linear_output2.value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-814b7d72022f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
