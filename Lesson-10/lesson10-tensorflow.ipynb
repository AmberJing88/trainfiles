{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson-10 TensorFlow practise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic data opearation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sess.run()的第一个参数是一般是[optimizer, cost]，这样返回值也是optimizer, cost的结果，相当于有几个参数就返回几个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 2, b = 3\n",
      "Addation with constant 5\n"
     ]
    }
   ],
   "source": [
    "# lunch the default graph\n",
    "with tf.Session() as sess:\n",
    "    print('a = {}, b = {}'.format(sess.run(a), sess.run(b)))\n",
    "    print('Addation with constant {}'.format(sess.run(a + b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Operations with variable as graph input\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Variable op. (define as input when running session)\n",
    "# tf Graph input\n",
    "#类似初始化一个变量，指定类型，但不赋值，而是通过feed_dict传入值\n",
    "#可以把它看成一个神经节点对象，节点的数值里保存了其值，在训练过程中其值会更新，但初始值一定要通过feedfeed_dict传入\n",
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addation with variables : 8\n"
     ]
    }
   ],
   "source": [
    "# define some operations\n",
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)\n",
    "with tf.Session() as sess:\n",
    "    print('Addation with variables : {}'.format(sess.run(add, feed_dict={a:5, b:3})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager API（实时执行，不通过tf.Session()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set eager mode...\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1bbf1468a402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#assert tf.executing_eagerly()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "#set Eager API\n",
    "print('set eager mode...')\n",
    "tf.enable_eager_execution()\n",
    "#assert tf.executing_eagerly()\n",
    "tfe = tf.contrib.eager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#define constant tensors\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "c = a +b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixing operation with tensors and numpy arrays\n",
      "Tensor:\n",
      " a = tf.Tensor(\n",
      "[[2. 1.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "NumpyArray:\n",
      " b = [[2. 5.]\n",
      " [3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#full compatibility with numpy\n",
    "print('mixing operation with tensors and numpy arrays')\n",
    "#define a constant tensor\n",
    "a = tf.constant([[2., 1.],\n",
    "                [3., 4.]], dtype=tf.float32)\n",
    "print(\"Tensor:\\n a = %s\" % a)\n",
    "b = np.array([[2., 5.],\n",
    "             [3., 1.]], dtype=np.float32)\n",
    "print(\"NumpyArray:\\n b = %s\" % b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running operations, without tf.Session\n",
      "a + b = tf.Tensor(\n",
      "[[4. 6.]\n",
      " [6. 5.]], shape=(2, 2), dtype=float32)\n",
      "a * b = tf.Tensor(\n",
      "[[ 7. 11.]\n",
      " [18. 19.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#run the operation without session.run()\n",
    "print(\"Running operations, without tf.Session\")\n",
    "c = a + b\n",
    "print(\"a + b = %s\" % c)\n",
    "\n",
    "d = tf.matmul(a, b)\n",
    "print(\"a * b = %s\" % d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regrassion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "rng = numpy.random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "learning_rate = 0.01\n",
    "training_epoch = 1000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input graph\n",
    "X = tf.placeholder('float')\n",
    "Y = tf.placeholder('float')\n",
    "#set model weights\n",
    "W = tf.Variable(rng.randn(), name='weights')\n",
    "b= tf.Variable(rng.randn(), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct a linear model wx + b\n",
    "pred = tf.add(tf.multiply(W, X), b)\n",
    "#mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred - y, 2)) / (2 * n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "#initialize the variable\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0050 cost= 0.002171067 W= 0.026237627 b= 1.5127242\n",
      "epoch: 0050 cost= 0.002170006 W= 0.026423542 b= 1.5127664\n",
      "epoch: 0050 cost= 0.002170393 W= 0.026559114 b= 1.512791\n",
      "epoch: 0050 cost= 0.002170662 W= 0.026594628 b= 1.5127963\n",
      "epoch: 0050 cost= 0.002170766 W= 0.026606461 b= 1.512798\n",
      "epoch: 0050 cost= 0.002172894 W= 0.026793547 b= 1.5128429\n",
      "epoch: 0050 cost= 0.002169106 W= 0.026362943 b= 1.5127989\n",
      "epoch: 0050 cost= 0.002169075 W= 0.026451038 b= 1.5128132\n",
      "epoch: 0050 cost= 0.002169017 W= 0.026390424 b= 1.5128052\n",
      "epoch: 0050 cost= 0.002168390 W= 0.026556144 b= 1.5128818\n",
      "epoch: 0050 cost= 0.002168392 W= 0.026556598 b= 1.5128819\n",
      "epoch: 0050 cost= 0.002172136 W= 0.0259253 b= 1.5128233\n",
      "epoch: 0050 cost= 0.002169181 W= 0.0260798 b= 1.5128524\n",
      "epoch: 0050 cost= 0.002170821 W= 0.025979072 b= 1.5128398\n",
      "epoch: 0050 cost= 0.002168575 W= 0.02611302 b= 1.5128635\n",
      "epoch: 0050 cost= 0.002174207 W= 0.025813485 b= 1.5128312\n",
      "epoch: 0050 cost= 0.002169013 W= 0.02600887 b= 1.5128943\n",
      "epoch: 0100 cost= 0.001920413 W= 0.024676643 b= 1.5238663\n",
      "epoch: 0100 cost= 0.001919475 W= 0.024851495 b= 1.523906\n",
      "epoch: 0100 cost= 0.001919818 W= 0.024979 b= 1.5239291\n",
      "epoch: 0100 cost= 0.001920054 W= 0.0250124 b= 1.5239341\n",
      "epoch: 0100 cost= 0.001920147 W= 0.02502353 b= 1.5239357\n",
      "epoch: 0100 cost= 0.001922032 W= 0.025199484 b= 1.5239779\n",
      "epoch: 0100 cost= 0.001918682 W= 0.024794498 b= 1.5239365\n",
      "epoch: 0100 cost= 0.001918655 W= 0.024877353 b= 1.5239499\n",
      "epoch: 0100 cost= 0.001918603 W= 0.024820343 b= 1.5239424\n",
      "epoch: 0100 cost= 0.001918052 W= 0.024976203 b= 1.5240142\n",
      "epoch: 0100 cost= 0.001918054 W= 0.024976632 b= 1.5240144\n",
      "epoch: 0100 cost= 0.001921366 W= 0.024382891 b= 1.5239593\n",
      "epoch: 0100 cost= 0.001918754 W= 0.0245282 b= 1.5239866\n",
      "epoch: 0100 cost= 0.001920202 W= 0.024433466 b= 1.5239748\n",
      "epoch: 0100 cost= 0.001918213 W= 0.024559446 b= 1.5239971\n",
      "epoch: 0100 cost= 0.001923198 W= 0.02427773 b= 1.5239667\n",
      "epoch: 0100 cost= 0.001918604 W= 0.024461491 b= 1.5240259\n",
      "epoch: 0150 cost= 0.001698801 W= 0.023209102 b= 1.5343405\n",
      "epoch: 0150 cost= 0.001697968 W= 0.023373557 b= 1.5343779\n",
      "epoch: 0150 cost= 0.001698270 W= 0.023493482 b= 1.5343997\n",
      "epoch: 0150 cost= 0.001698481 W= 0.023524897 b= 1.5344044\n",
      "epoch: 0150 cost= 0.001698560 W= 0.023535365 b= 1.534406\n",
      "epoch: 0150 cost= 0.001700227 W= 0.023700856 b= 1.5344456\n",
      "epoch: 0150 cost= 0.001697267 W= 0.023319956 b= 1.5344067\n",
      "epoch: 0150 cost= 0.001697242 W= 0.023397883 b= 1.5344193\n",
      "epoch: 0150 cost= 0.001697195 W= 0.023344265 b= 1.5344123\n",
      "epoch: 0150 cost= 0.001696708 W= 0.023490857 b= 1.5344799\n",
      "epoch: 0150 cost= 0.001696710 W= 0.023491262 b= 1.5344799\n",
      "epoch: 0150 cost= 0.001699641 W= 0.022932833 b= 1.5344281\n",
      "epoch: 0150 cost= 0.001697328 W= 0.0230695 b= 1.5344539\n",
      "epoch: 0150 cost= 0.001698609 W= 0.022980401 b= 1.5344428\n",
      "epoch: 0150 cost= 0.001696850 W= 0.02309889 b= 1.5344638\n",
      "epoch: 0150 cost= 0.001701259 W= 0.022833928 b= 1.5344352\n",
      "epoch: 0150 cost= 0.001697194 W= 0.023006761 b= 1.534491\n",
      "epoch: 0200 cost= 0.001502799 W= 0.021829197 b= 1.5441899\n",
      "epoch: 0200 cost= 0.001502063 W= 0.021983875 b= 1.5442251\n",
      "epoch: 0200 cost= 0.001502330 W= 0.02209667 b= 1.5442456\n",
      "epoch: 0200 cost= 0.001502516 W= 0.022126216 b= 1.54425\n",
      "epoch: 0200 cost= 0.001502588 W= 0.022136062 b= 1.5442514\n",
      "epoch: 0200 cost= 0.001504063 W= 0.022291714 b= 1.5442888\n",
      "epoch: 0200 cost= 0.001501441 W= 0.021933462 b= 1.5442522\n",
      "epoch: 0200 cost= 0.001501421 W= 0.022006756 b= 1.544264\n",
      "epoch: 0200 cost= 0.001501381 W= 0.021956326 b= 1.5442573\n",
      "epoch: 0200 cost= 0.001500947 W= 0.022094203 b= 1.544321\n",
      "epoch: 0200 cost= 0.001500950 W= 0.022094583 b= 1.544321\n",
      "epoch: 0200 cost= 0.001503543 W= 0.021569356 b= 1.5442723\n",
      "epoch: 0200 cost= 0.001501497 W= 0.0216979 b= 1.5442965\n",
      "epoch: 0200 cost= 0.001502631 W= 0.021614097 b= 1.544286\n",
      "epoch: 0200 cost= 0.001501077 W= 0.021725541 b= 1.5443057\n",
      "epoch: 0200 cost= 0.001504977 W= 0.021476334 b= 1.5442787\n",
      "epoch: 0200 cost= 0.001501382 W= 0.021638893 b= 1.5443312\n",
      "epoch: 0250 cost= 0.001329397 W= 0.020531248 b= 1.5534545\n",
      "epoch: 0250 cost= 0.001328747 W= 0.020676728 b= 1.5534875\n",
      "epoch: 0250 cost= 0.001328984 W= 0.020782815 b= 1.5535069\n",
      "epoch: 0250 cost= 0.001329147 W= 0.020810604 b= 1.553511\n",
      "epoch: 0250 cost= 0.001329211 W= 0.020819863 b= 1.5535123\n",
      "epoch: 0250 cost= 0.001330515 W= 0.02096626 b= 1.5535475\n",
      "epoch: 0250 cost= 0.001328196 W= 0.020629307 b= 1.553513\n",
      "epoch: 0250 cost= 0.001328176 W= 0.020698242 b= 1.5535243\n",
      "epoch: 0250 cost= 0.001328139 W= 0.02065081 b= 1.553518\n",
      "epoch: 0250 cost= 0.001327756 W= 0.020780487 b= 1.5535779\n",
      "epoch: 0250 cost= 0.001327759 W= 0.020780843 b= 1.5535779\n",
      "epoch: 0250 cost= 0.001330051 W= 0.020286845 b= 1.5535321\n",
      "epoch: 0250 cost= 0.001328243 W= 0.020407744 b= 1.5535549\n",
      "epoch: 0250 cost= 0.001329246 W= 0.020328924 b= 1.553545\n",
      "epoch: 0250 cost= 0.001327869 W= 0.02043374 b= 1.5535636\n",
      "epoch: 0250 cost= 0.001331319 W= 0.02019935 b= 1.5535383\n",
      "epoch: 0250 cost= 0.001328139 W= 0.02035224 b= 1.5535877\n",
      "epoch: 0300 cost= 0.001175997 W= 0.019310385 b= 1.5621686\n",
      "epoch: 0300 cost= 0.001175421 W= 0.019447215 b= 1.5621997\n",
      "epoch: 0300 cost= 0.001175632 W= 0.019546993 b= 1.5622178\n",
      "epoch: 0300 cost= 0.001175775 W= 0.01957313 b= 1.5622218\n",
      "epoch: 0300 cost= 0.001175830 W= 0.01958184 b= 1.5622231\n",
      "epoch: 0300 cost= 0.001176985 W= 0.01971953 b= 1.5622561\n",
      "epoch: 0300 cost= 0.001174934 W= 0.019402614 b= 1.5622237\n",
      "epoch: 0300 cost= 0.001174918 W= 0.01946745 b= 1.5622342\n",
      "epoch: 0300 cost= 0.001174885 W= 0.019422838 b= 1.5622283\n",
      "epoch: 0300 cost= 0.001174547 W= 0.019544804 b= 1.5622846\n",
      "epoch: 0300 cost= 0.001174549 W= 0.01954514 b= 1.5622846\n",
      "epoch: 0300 cost= 0.001176576 W= 0.019080516 b= 1.5622416\n",
      "epoch: 0300 cost= 0.001174976 W= 0.019194227 b= 1.562263\n",
      "epoch: 0300 cost= 0.001175864 W= 0.019120093 b= 1.5622537\n",
      "epoch: 0300 cost= 0.001174647 W= 0.019218678 b= 1.5622711\n",
      "epoch: 0300 cost= 0.001177697 W= 0.018998226 b= 1.5622474\n",
      "epoch: 0300 cost= 0.001174885 W= 0.019142026 b= 1.5622938\n",
      "epoch: 0350 cost= 0.001040331 W= 0.018162409 b= 1.5703624\n",
      "epoch: 0350 cost= 0.001039823 W= 0.018291105 b= 1.5703917\n",
      "epoch: 0350 cost= 0.001040008 W= 0.018384952 b= 1.5704087\n",
      "epoch: 0350 cost= 0.001040136 W= 0.018409535 b= 1.5704124\n",
      "epoch: 0350 cost= 0.001040185 W= 0.018417727 b= 1.5704136\n",
      "epoch: 0350 cost= 0.001041205 W= 0.018547233 b= 1.5704447\n",
      "epoch: 0350 cost= 0.001039392 W= 0.018249158 b= 1.5704142\n",
      "epoch: 0350 cost= 0.001039377 W= 0.018310139 b= 1.5704241\n",
      "epoch: 0350 cost= 0.001039348 W= 0.01826818 b= 1.5704186\n",
      "epoch: 0350 cost= 0.001039049 W= 0.018382896 b= 1.5704715\n",
      "epoch: 0350 cost= 0.001039050 W= 0.01838321 b= 1.5704715\n",
      "epoch: 0350 cost= 0.001040846 W= 0.01794621 b= 1.570431\n",
      "epoch: 0350 cost= 0.001039429 W= 0.01805316 b= 1.5704511\n",
      "epoch: 0350 cost= 0.001040213 W= 0.017983433 b= 1.5704424\n",
      "epoch: 0350 cost= 0.001039136 W= 0.018076155 b= 1.5704589\n",
      "epoch: 0350 cost= 0.001041837 W= 0.017868808 b= 1.5704365\n",
      "epoch: 0350 cost= 0.001039348 W= 0.01800406 b= 1.5704801\n",
      "epoch: 0400 cost= 0.000920293 W= 0.017082434 b= 1.5780708\n",
      "epoch: 0400 cost= 0.000919841 W= 0.017203476 b= 1.5780983\n",
      "epoch: 0400 cost= 0.000920005 W= 0.017291743 b= 1.5781144\n",
      "epoch: 0400 cost= 0.000920117 W= 0.017314866 b= 1.5781178\n",
      "epoch: 0400 cost= 0.000920162 W= 0.017322572 b= 1.5781189\n",
      "epoch: 0400 cost= 0.000921065 W= 0.017444378 b= 1.5781481\n",
      "epoch: 0400 cost= 0.000919459 W= 0.017164027 b= 1.5781195\n",
      "epoch: 0400 cost= 0.000919447 W= 0.017221384 b= 1.5781288\n",
      "epoch: 0400 cost= 0.000919422 W= 0.01718192 b= 1.5781236\n",
      "epoch: 0400 cost= 0.000919157 W= 0.017289815 b= 1.5781734\n",
      "epoch: 0400 cost= 0.000919158 W= 0.017290113 b= 1.5781734\n",
      "epoch: 0400 cost= 0.000920747 W= 0.016879097 b= 1.5781353\n",
      "epoch: 0400 cost= 0.000919493 W= 0.016979687 b= 1.5781542\n",
      "epoch: 0400 cost= 0.000920189 W= 0.016914109 b= 1.578146\n",
      "epoch: 0400 cost= 0.000919237 W= 0.01700132 b= 1.5781614\n",
      "epoch: 0400 cost= 0.000921624 W= 0.016806303 b= 1.5781404\n",
      "epoch: 0400 cost= 0.000919424 W= 0.016933512 b= 1.5781814\n",
      "epoch: 0450 cost= 0.000814099 W= 0.016066678 b= 1.5853211\n",
      "epoch: 0450 cost= 0.000813701 W= 0.016180523 b= 1.5853469\n",
      "epoch: 0450 cost= 0.000813845 W= 0.01626354 b= 1.5853621\n",
      "epoch: 0450 cost= 0.000813945 W= 0.016285287 b= 1.5853653\n",
      "epoch: 0450 cost= 0.000813984 W= 0.016292535 b= 1.5853664\n",
      "epoch: 0450 cost= 0.000814782 W= 0.016407097 b= 1.5853939\n",
      "epoch: 0450 cost= 0.000813363 W= 0.016143415 b= 1.585367\n",
      "epoch: 0450 cost= 0.000813351 W= 0.016197361 b= 1.5853757\n",
      "epoch: 0450 cost= 0.000813329 W= 0.016160244 b= 1.5853708\n",
      "epoch: 0450 cost= 0.000813095 W= 0.016261723 b= 1.5854176\n",
      "epoch: 0450 cost= 0.000813095 W= 0.016262002 b= 1.5854176\n",
      "epoch: 0450 cost= 0.000814501 W= 0.015875425 b= 1.5853817\n",
      "epoch: 0450 cost= 0.000813393 W= 0.015970035 b= 1.5853995\n",
      "epoch: 0450 cost= 0.000814009 W= 0.015908355 b= 1.5853918\n",
      "epoch: 0450 cost= 0.000813165 W= 0.01599038 b= 1.5854063\n",
      "epoch: 0450 cost= 0.000815278 W= 0.01580696 b= 1.5853865\n",
      "epoch: 0450 cost= 0.000813330 W= 0.015926605 b= 1.5854251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0500 cost= 0.000720167 W= 0.015111383 b= 1.5921397\n",
      "epoch: 0500 cost= 0.000719814 W= 0.015218459 b= 1.592164\n",
      "epoch: 0500 cost= 0.000719943 W= 0.015296541 b= 1.5921782\n",
      "epoch: 0500 cost= 0.000720031 W= 0.015316995 b= 1.5921813\n",
      "epoch: 0500 cost= 0.000720066 W= 0.01532381 b= 1.5921823\n",
      "epoch: 0500 cost= 0.000720772 W= 0.015431561 b= 1.5922081\n",
      "epoch: 0500 cost= 0.000719517 W= 0.015183558 b= 1.5921828\n",
      "epoch: 0500 cost= 0.000719506 W= 0.015234295 b= 1.592191\n",
      "epoch: 0500 cost= 0.000719488 W= 0.015199385 b= 1.5921863\n",
      "epoch: 0500 cost= 0.000719281 W= 0.01529483 b= 1.5922303\n",
      "epoch: 0500 cost= 0.000719282 W= 0.015295094 b= 1.5922303\n",
      "epoch: 0500 cost= 0.000720525 W= 0.014931504 b= 1.5921966\n",
      "epoch: 0500 cost= 0.000719546 W= 0.015020489 b= 1.5922133\n",
      "epoch: 0500 cost= 0.000720090 W= 0.014962478 b= 1.592206\n",
      "epoch: 0500 cost= 0.000719345 W= 0.015039626 b= 1.5922196\n",
      "epoch: 0500 cost= 0.000721214 W= 0.014867112 b= 1.592201\n",
      "epoch: 0500 cost= 0.000719490 W= 0.014979644 b= 1.5922374\n",
      "epoch: 0550 cost= 0.000637080 W= 0.014212966 b= 1.5985523\n",
      "epoch: 0550 cost= 0.000636768 W= 0.014313675 b= 1.5985752\n",
      "epoch: 0550 cost= 0.000636882 W= 0.014387115 b= 1.5985886\n",
      "epoch: 0550 cost= 0.000636960 W= 0.014406352 b= 1.5985914\n",
      "epoch: 0550 cost= 0.000636991 W= 0.014412763 b= 1.5985924\n",
      "epoch: 0550 cost= 0.000637616 W= 0.014514107 b= 1.5986167\n",
      "epoch: 0550 cost= 0.000636504 W= 0.014280849 b= 1.5985929\n",
      "epoch: 0550 cost= 0.000636495 W= 0.01432857 b= 1.5986006\n",
      "epoch: 0550 cost= 0.000636477 W= 0.014295735 b= 1.5985963\n",
      "epoch: 0550 cost= 0.000636293 W= 0.0143855065 b= 1.5986378\n",
      "epoch: 0550 cost= 0.000636295 W= 0.014385753 b= 1.5986378\n",
      "epoch: 0550 cost= 0.000637393 W= 0.014043777 b= 1.5986061\n",
      "epoch: 0550 cost= 0.000636527 W= 0.014127471 b= 1.5986218\n",
      "epoch: 0550 cost= 0.000637007 W= 0.014072906 b= 1.598615\n",
      "epoch: 0550 cost= 0.000636347 W= 0.0141454665 b= 1.5986279\n",
      "epoch: 0550 cost= 0.000638000 W= 0.013983207 b= 1.5986104\n",
      "epoch: 0550 cost= 0.000636477 W= 0.014089048 b= 1.5986445\n",
      "epoch: 0600 cost= 0.000563566 W= 0.013367816 b= 1.6045848\n",
      "epoch: 0600 cost= 0.000563290 W= 0.013462537 b= 1.6046064\n",
      "epoch: 0600 cost= 0.000563391 W= 0.013531609 b= 1.6046189\n",
      "epoch: 0600 cost= 0.000563460 W= 0.013549703 b= 1.6046216\n",
      "epoch: 0600 cost= 0.000563487 W= 0.013555732 b= 1.6046225\n",
      "epoch: 0600 cost= 0.000564040 W= 0.01365105 b= 1.6046454\n",
      "epoch: 0600 cost= 0.000563057 W= 0.013431661 b= 1.604623\n",
      "epoch: 0600 cost= 0.000563049 W= 0.013476544 b= 1.6046302\n",
      "epoch: 0600 cost= 0.000563033 W= 0.013445661 b= 1.6046262\n",
      "epoch: 0600 cost= 0.000562870 W= 0.013530094 b= 1.6046652\n",
      "epoch: 0600 cost= 0.000562872 W= 0.013530326 b= 1.6046652\n",
      "epoch: 0600 cost= 0.000563843 W= 0.013208685 b= 1.6046354\n",
      "epoch: 0600 cost= 0.000563077 W= 0.013287402 b= 1.6046501\n",
      "epoch: 0600 cost= 0.000563502 W= 0.013236082 b= 1.6046437\n",
      "epoch: 0600 cost= 0.000562919 W= 0.013304328 b= 1.6046557\n",
      "epoch: 0600 cost= 0.000564382 W= 0.013151717 b= 1.6046393\n",
      "epoch: 0600 cost= 0.000563035 W= 0.013251265 b= 1.6046714\n",
      "epoch: 0650 cost= 0.000498547 W= 0.012573041 b= 1.6102575\n",
      "epoch: 0650 cost= 0.000498303 W= 0.012662131 b= 1.6102778\n",
      "epoch: 0650 cost= 0.000498392 W= 0.012727098 b= 1.6102896\n",
      "epoch: 0650 cost= 0.000498453 W= 0.012744116 b= 1.6102921\n",
      "epoch: 0650 cost= 0.000498478 W= 0.012749787 b= 1.6102929\n",
      "epoch: 0650 cost= 0.000498966 W= 0.012839438 b= 1.6103144\n",
      "epoch: 0650 cost= 0.000498098 W= 0.012633095 b= 1.6102933\n",
      "epoch: 0650 cost= 0.000498092 W= 0.0126753105 b= 1.6103001\n",
      "epoch: 0650 cost= 0.000498078 W= 0.012646264 b= 1.6102962\n",
      "epoch: 0650 cost= 0.000497934 W= 0.012725677 b= 1.6103328\n",
      "epoch: 0650 cost= 0.000497935 W= 0.012725896 b= 1.6103328\n",
      "epoch: 0650 cost= 0.000498795 W= 0.01242338 b= 1.6103048\n",
      "epoch: 0650 cost= 0.000498117 W= 0.012497418 b= 1.6103188\n",
      "epoch: 0650 cost= 0.000498494 W= 0.012449151 b= 1.6103127\n",
      "epoch: 0650 cost= 0.000497978 W= 0.0125133395 b= 1.610324\n",
      "epoch: 0650 cost= 0.000499272 W= 0.012369804 b= 1.6103085\n",
      "epoch: 0650 cost= 0.000498079 W= 0.012463434 b= 1.6103387\n",
      "epoch: 0700 cost= 0.000441015 W= 0.011825343 b= 1.6155944\n",
      "epoch: 0700 cost= 0.000440798 W= 0.011909135 b= 1.6156135\n",
      "epoch: 0700 cost= 0.000440877 W= 0.011970238 b= 1.6156245\n",
      "epoch: 0700 cost= 0.000440932 W= 0.0119862445 b= 1.6156269\n",
      "epoch: 0700 cost= 0.000440953 W= 0.011991578 b= 1.6156276\n",
      "epoch: 0700 cost= 0.000441385 W= 0.012075898 b= 1.6156479\n",
      "epoch: 0700 cost= 0.000440616 W= 0.011881825 b= 1.6156281\n",
      "epoch: 0700 cost= 0.000440609 W= 0.01192153 b= 1.6156346\n",
      "epoch: 0700 cost= 0.000440597 W= 0.01189421 b= 1.615631\n",
      "epoch: 0700 cost= 0.000440470 W= 0.0119689 b= 1.6156654\n",
      "epoch: 0700 cost= 0.000440471 W= 0.011969106 b= 1.6156654\n",
      "epoch: 0700 cost= 0.000441232 W= 0.01168458 b= 1.6156391\n",
      "epoch: 0700 cost= 0.000440631 W= 0.011754214 b= 1.6156522\n",
      "epoch: 0700 cost= 0.000440965 W= 0.0117088165 b= 1.6156465\n",
      "epoch: 0700 cost= 0.000440507 W= 0.011769188 b= 1.6156572\n",
      "epoch: 0700 cost= 0.000441652 W= 0.011634187 b= 1.6156427\n",
      "epoch: 0700 cost= 0.000440598 W= 0.011722248 b= 1.615671\n",
      "epoch: 0750 cost= 0.000390143 W= 0.011122428 b= 1.6206117\n",
      "epoch: 0750 cost= 0.000389952 W= 0.011201239 b= 1.6206295\n",
      "epoch: 0750 cost= 0.000390021 W= 0.01125871 b= 1.62064\n",
      "epoch: 0750 cost= 0.000390069 W= 0.011273765 b= 1.6206423\n",
      "epoch: 0750 cost= 0.000390088 W= 0.011278781 b= 1.620643\n",
      "epoch: 0750 cost= 0.000390470 W= 0.011358089 b= 1.6206621\n",
      "epoch: 0750 cost= 0.000389790 W= 0.0111755505 b= 1.6206434\n",
      "epoch: 0750 cost= 0.000389784 W= 0.011212895 b= 1.6206495\n",
      "epoch: 0750 cost= 0.000389773 W= 0.011187199 b= 1.6206461\n",
      "epoch: 0750 cost= 0.000389661 W= 0.011257449 b= 1.6206785\n",
      "epoch: 0750 cost= 0.000389661 W= 0.011257642 b= 1.6206785\n",
      "epoch: 0750 cost= 0.000390334 W= 0.010990027 b= 1.6206537\n",
      "epoch: 0750 cost= 0.000389804 W= 0.011055522 b= 1.620666\n",
      "epoch: 0750 cost= 0.000390099 W= 0.011012822 b= 1.6206607\n",
      "epoch: 0750 cost= 0.000389695 W= 0.011069605 b= 1.6206707\n",
      "epoch: 0750 cost= 0.000390708 W= 0.010942629 b= 1.620657\n",
      "epoch: 0750 cost= 0.000389775 W= 0.011025455 b= 1.6206837\n",
      "epoch: 0800 cost= 0.000345119 W= 0.010460987 b= 1.6253328\n",
      "epoch: 0800 cost= 0.000344951 W= 0.010535111 b= 1.6253496\n",
      "epoch: 0800 cost= 0.000345013 W= 0.010589164 b= 1.6253594\n",
      "epoch: 0800 cost= 0.000345055 W= 0.010603324 b= 1.6253616\n",
      "epoch: 0800 cost= 0.000345072 W= 0.010608042 b= 1.6253623\n",
      "epoch: 0800 cost= 0.000345410 W= 0.010682633 b= 1.6253802\n",
      "epoch: 0800 cost= 0.000344808 W= 0.01051095 b= 1.6253626\n",
      "epoch: 0800 cost= 0.000344803 W= 0.010546074 b= 1.6253684\n",
      "epoch: 0800 cost= 0.000344794 W= 0.010521906 b= 1.6253651\n",
      "epoch: 0800 cost= 0.000344694 W= 0.010587979 b= 1.6253957\n",
      "epoch: 0800 cost= 0.000344695 W= 0.010588161 b= 1.6253957\n",
      "epoch: 0800 cost= 0.000345291 W= 0.010336461 b= 1.6253723\n",
      "epoch: 0800 cost= 0.000344821 W= 0.010398061 b= 1.6253839\n",
      "epoch: 0800 cost= 0.000345081 W= 0.010357901 b= 1.6253788\n",
      "epoch: 0800 cost= 0.000344724 W= 0.010411307 b= 1.6253883\n",
      "epoch: 0800 cost= 0.000345620 W= 0.010291882 b= 1.6253754\n",
      "epoch: 0800 cost= 0.000344794 W= 0.010369782 b= 1.6254005\n",
      "epoch: 0850 cost= 0.000305297 W= 0.009838982 b= 1.6297727\n",
      "epoch: 0850 cost= 0.000305147 W= 0.009908698 b= 1.6297885\n",
      "epoch: 0850 cost= 0.000305201 W= 0.009959536 b= 1.6297978\n",
      "epoch: 0850 cost= 0.000305239 W= 0.009972852 b= 1.6297998\n",
      "epoch: 0850 cost= 0.000305254 W= 0.009977289 b= 1.6298004\n",
      "epoch: 0850 cost= 0.000305553 W= 0.010047444 b= 1.6298172\n",
      "epoch: 0850 cost= 0.000305022 W= 0.009885969 b= 1.6298007\n",
      "epoch: 0850 cost= 0.000305017 W= 0.009919004 b= 1.629806\n",
      "epoch: 0850 cost= 0.000305008 W= 0.009896273 b= 1.6298031\n",
      "epoch: 0850 cost= 0.000304921 W= 0.009958417 b= 1.6298318\n",
      "epoch: 0850 cost= 0.000304921 W= 0.009958588 b= 1.6298318\n",
      "epoch: 0850 cost= 0.000305447 W= 0.009721853 b= 1.6298099\n",
      "epoch: 0850 cost= 0.000305033 W= 0.0097797895 b= 1.6298207\n",
      "epoch: 0850 cost= 0.000305263 W= 0.009742018 b= 1.6298159\n",
      "epoch: 0850 cost= 0.000304947 W= 0.009792248 b= 1.6298249\n",
      "epoch: 0850 cost= 0.000305739 W= 0.009679923 b= 1.6298127\n",
      "epoch: 0850 cost= 0.000305009 W= 0.009753192 b= 1.6298363\n",
      "epoch: 0900 cost= 0.000270090 W= 0.009254226 b= 1.6339461\n",
      "epoch: 0900 cost= 0.000269957 W= 0.009319801 b= 1.633961\n",
      "epoch: 0900 cost= 0.000270005 W= 0.009367619 b= 1.6339697\n",
      "epoch: 0900 cost= 0.000270038 W= 0.009380146 b= 1.6339716\n",
      "epoch: 0900 cost= 0.000270051 W= 0.00938432 b= 1.6339722\n",
      "epoch: 0900 cost= 0.000270316 W= 0.009450307 b= 1.633988\n",
      "epoch: 0900 cost= 0.000269845 W= 0.009298431 b= 1.6339725\n",
      "epoch: 0900 cost= 0.000269841 W= 0.0093295025 b= 1.6339775\n",
      "epoch: 0900 cost= 0.000269834 W= 0.009308124 b= 1.6339747\n",
      "epoch: 0900 cost= 0.000269756 W= 0.009366576 b= 1.6340016\n",
      "epoch: 0900 cost= 0.000269757 W= 0.009366737 b= 1.6340016\n",
      "epoch: 0900 cost= 0.000270223 W= 0.009144073 b= 1.633981\n",
      "epoch: 0900 cost= 0.000269855 W= 0.009198568 b= 1.6339912\n",
      "epoch: 0900 cost= 0.000270059 W= 0.009163042 b= 1.6339868\n",
      "epoch: 0900 cost= 0.000269779 W= 0.009210287 b= 1.6339952\n",
      "epoch: 0900 cost= 0.000270481 W= 0.009104638 b= 1.6339837\n",
      "epoch: 0900 cost= 0.000269836 W= 0.009173553 b= 1.6340059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0950 cost= 0.000238918 W= 0.008703915 b= 1.6378745\n",
      "epoch: 0950 cost= 0.000238801 W= 0.0087655885 b= 1.6378886\n",
      "epoch: 0950 cost= 0.000238844 W= 0.008810561 b= 1.6378968\n",
      "epoch: 0950 cost= 0.000238873 W= 0.0088223405 b= 1.6378986\n",
      "epoch: 0950 cost= 0.000238884 W= 0.008826264 b= 1.6378992\n",
      "epoch: 0950 cost= 0.000239119 W= 0.008888326 b= 1.6379141\n",
      "epoch: 0950 cost= 0.000238703 W= 0.008745478 b= 1.6378994\n",
      "epoch: 0950 cost= 0.000238699 W= 0.008774701 b= 1.6379042\n",
      "epoch: 0950 cost= 0.000238692 W= 0.008754591 b= 1.6379015\n",
      "epoch: 0950 cost= 0.000238623 W= 0.0088095665 b= 1.6379269\n",
      "epoch: 0950 cost= 0.000238624 W= 0.008809716 b= 1.6379269\n",
      "epoch: 0950 cost= 0.000239035 W= 0.008600291 b= 1.6379075\n",
      "epoch: 0950 cost= 0.000238710 W= 0.008651543 b= 1.6379172\n",
      "epoch: 0950 cost= 0.000238890 W= 0.008618128 b= 1.637913\n",
      "epoch: 0950 cost= 0.000238643 W= 0.008662562 b= 1.6379209\n",
      "epoch: 0950 cost= 0.000239263 W= 0.008563194 b= 1.6379101\n",
      "epoch: 0950 cost= 0.000238692 W= 0.00862801 b= 1.637931\n",
      "epoch: 1000 cost= 0.000211338 W= 0.008186055 b= 1.6415703\n",
      "epoch: 1000 cost= 0.000211234 W= 0.008244061 b= 1.6415836\n",
      "epoch: 1000 cost= 0.000211272 W= 0.00828636 b= 1.6415913\n",
      "epoch: 1000 cost= 0.000211297 W= 0.008297441 b= 1.641593\n",
      "epoch: 1000 cost= 0.000211308 W= 0.008301133 b= 1.6415935\n",
      "epoch: 1000 cost= 0.000211516 W= 0.008359504 b= 1.6416074\n",
      "epoch: 1000 cost= 0.000211146 W= 0.008225158 b= 1.6415937\n",
      "epoch: 1000 cost= 0.000211144 W= 0.008252644 b= 1.6415981\n",
      "epoch: 1000 cost= 0.000211138 W= 0.0082337335 b= 1.6415956\n",
      "epoch: 1000 cost= 0.000211077 W= 0.008285438 b= 1.6416194\n",
      "epoch: 1000 cost= 0.000211078 W= 0.008285581 b= 1.6416194\n",
      "epoch: 1000 cost= 0.000211443 W= 0.008088619 b= 1.6416012\n",
      "epoch: 1000 cost= 0.000211155 W= 0.008136824 b= 1.6416103\n",
      "epoch: 1000 cost= 0.000211315 W= 0.008105398 b= 1.6416063\n",
      "epoch: 1000 cost= 0.000211096 W= 0.00814719 b= 1.6416137\n",
      "epoch: 1000 cost= 0.000211645 W= 0.008053737 b= 1.6416036\n",
      "epoch: 1000 cost= 0.000211139 W= 0.008114697 b= 1.6416233\n",
      "optimization finished!\n",
      "training_cost= 0.00021113898 W= 0.008114697 b= 1.6416233 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #fit all training data\n",
    "    for epoch in range(training_epoch):\n",
    "        for x, y in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X:x, Y:y})\n",
    "            \n",
    "            #display each epoch step\n",
    "            if (epoch +1) % display_step ==0:\n",
    "                c= sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "                print('epoch:', '%04d' % (epoch+1), 'cost=', '{:.9f}'.format(c),\n",
    "                     'W=', sess.run(W), 'b=', sess.run(b))\n",
    "    print('optimization finished!')\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print('training_cost=', training_cost, 'W=', sess.run(W), 'b=', sess.run(b), '\\n')\n",
    "    \n",
    "    # graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original Data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regreesion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-14-1f2fc8567ccc>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/student/project/project-01/nlp_bots/hw-xujing/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/student/project/project-01/nlp_bots/hw-xujing/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/student/project/project-01/nlp_bots/hw-xujing/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/student/project/project-01/nlp_bots/hw-xujing/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data_path = '/home/student/project/project-01/nlp_bots/hw-xujing'\n",
    "mnist = input_data.read_data_sets(data_path, one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (55000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display data images\n",
    "#load data\n",
    "train_X = mnist.train.images  # training data\n",
    "valid_X = mnist.validation.images # validaton data\n",
    "test_X = mnist.test.images # test data\n",
    "#load labels\n",
    "train_Y = mnist.train.labels # train labels\n",
    "valid_Y = mnist.validation.labels # validation labels\n",
    "test_Y = mnist.test.labels # test labels\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=5, sharex='all', sharey='all') \n",
    "ax = ax.flatten()\n",
    "for i in range(20):\n",
    "    img = train_X[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "learning_rate = 0.01\n",
    "training_epoch = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "#tf graph input\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "# model variables\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # softmax\n",
    "\n",
    "#minimize error with cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "#gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001 cost= 1.183237542\n",
      "epoch: 0002 cost= 0.665535373\n",
      "epoch: 0003 cost= 0.552689176\n",
      "epoch: 0004 cost= 0.498705607\n",
      "epoch: 0005 cost= 0.465613799\n",
      "epoch: 0006 cost= 0.442399512\n",
      "epoch: 0007 cost= 0.425839814\n",
      "epoch: 0008 cost= 0.412213691\n",
      "epoch: 0009 cost= 0.401388979\n",
      "epoch: 0010 cost= 0.392320770\n",
      "epoch: 0011 cost= 0.384821161\n",
      "epoch: 0012 cost= 0.378168864\n",
      "epoch: 0013 cost= 0.372331645\n",
      "epoch: 0014 cost= 0.367483944\n",
      "epoch: 0015 cost= 0.362528441\n",
      "epoch: 0016 cost= 0.358694902\n",
      "epoch: 0017 cost= 0.354724888\n",
      "epoch: 0018 cost= 0.351422819\n",
      "epoch: 0019 cost= 0.348571076\n",
      "epoch: 0020 cost= 0.345251840\n",
      "epoch: 0021 cost= 0.342782933\n",
      "epoch: 0022 cost= 0.340284714\n",
      "epoch: 0023 cost= 0.337898877\n",
      "epoch: 0024 cost= 0.335535799\n",
      "epoch: 0025 cost= 0.333945199\n",
      "Optimization Done!\n",
      "Accuracy: 0.8893333\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #training cycle\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        #loop all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            #fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_xs, y:batch_ys})\n",
    "            #sess.run()\n",
    "            #compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        #display\n",
    "        if (epoch+1) % display_step ==0:\n",
    "            print('epoch:', '%04d' % (epoch+1), 'cost=', '{:.9f}'.format(avg_cost))\n",
    "    print('Optimization Done!')\n",
    "    \n",
    "    #test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # compute accuracy for 3000 samples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Accuracy:', accuracy.eval({x :mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/student/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-b7459f8c4195>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/student/project/project-01/nlp_bots/hw-xujing/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/student/project/project-01/nlp_bots/hw-xujing/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/student/project/project-01/nlp_bots/hw-xujing/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/student/project/project-01/nlp_bots/hw-xujing/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data_path = '/home/student/project/project-01/nlp_bots/hw-xujing'\n",
    "mnist = input_data.read_data_sets(data_path, one_hot=True)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = mnist.train.next_batch(5000)\n",
    "X_test, Y_test = mnist.test.next_batch(200)\n",
    "\n",
    "# input tf graph\n",
    "x_train = tf.placeholder('float', [None, 784])\n",
    "x_test = tf.placeholder('float', [784])\n",
    "\n",
    "#tf.negative()\n",
    "#nearest neighbor calculation using L1 distance\n",
    "#distance tf.reduce_sum(tf.abs(tf.add(x_train, tf.negative(x_test))), reduction_indices=1)\n",
    "#L2 distance\n",
    "distance = tf.reduce_sum(tf.square(tf.add(x_train, tf.negative(x_test))), reduction_indices= 1)\n",
    "#prediction get min distance index(nearest neighbor)\n",
    "pred = tf.argmin(distance, 0)\n",
    "\n",
    "accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0 prediction 1 true class 1\n",
      "test 1 prediction 0 true class 0\n",
      "test 2 prediction 1 true class 4\n",
      "test 3 prediction 0 true class 0\n",
      "test 4 prediction 6 true class 6\n",
      "test 5 prediction 1 true class 1\n",
      "test 6 prediction 7 true class 7\n",
      "test 7 prediction 3 true class 3\n",
      "test 8 prediction 2 true class 2\n",
      "test 9 prediction 9 true class 4\n",
      "test 10 prediction 2 true class 2\n",
      "test 11 prediction 1 true class 1\n",
      "test 12 prediction 4 true class 4\n",
      "test 13 prediction 9 true class 9\n",
      "test 14 prediction 4 true class 4\n",
      "test 15 prediction 2 true class 2\n",
      "test 16 prediction 9 true class 9\n",
      "test 17 prediction 4 true class 4\n",
      "test 18 prediction 2 true class 2\n",
      "test 19 prediction 2 true class 2\n",
      "test 20 prediction 8 true class 8\n",
      "test 21 prediction 6 true class 6\n",
      "test 22 prediction 7 true class 7\n",
      "test 23 prediction 2 true class 2\n",
      "test 24 prediction 2 true class 2\n",
      "test 25 prediction 2 true class 2\n",
      "test 26 prediction 0 true class 0\n",
      "test 27 prediction 3 true class 3\n",
      "test 28 prediction 6 true class 6\n",
      "test 29 prediction 0 true class 0\n",
      "test 30 prediction 4 true class 4\n",
      "test 31 prediction 1 true class 1\n",
      "test 32 prediction 0 true class 0\n",
      "test 33 prediction 4 true class 4\n",
      "test 34 prediction 2 true class 2\n",
      "test 35 prediction 4 true class 4\n",
      "test 36 prediction 3 true class 3\n",
      "test 37 prediction 9 true class 9\n",
      "test 38 prediction 3 true class 3\n",
      "test 39 prediction 8 true class 8\n",
      "test 40 prediction 4 true class 9\n",
      "test 41 prediction 2 true class 2\n",
      "test 42 prediction 8 true class 8\n",
      "test 43 prediction 2 true class 2\n",
      "test 44 prediction 1 true class 1\n",
      "test 45 prediction 4 true class 4\n",
      "test 46 prediction 9 true class 9\n",
      "test 47 prediction 8 true class 8\n",
      "test 48 prediction 1 true class 1\n",
      "test 49 prediction 3 true class 3\n",
      "test 50 prediction 1 true class 1\n",
      "test 51 prediction 0 true class 0\n",
      "test 52 prediction 6 true class 6\n",
      "test 53 prediction 6 true class 6\n",
      "test 54 prediction 3 true class 3\n",
      "test 55 prediction 2 true class 2\n",
      "test 56 prediction 5 true class 5\n",
      "test 57 prediction 0 true class 0\n",
      "test 58 prediction 9 true class 9\n",
      "test 59 prediction 6 true class 6\n",
      "test 60 prediction 3 true class 3\n",
      "test 61 prediction 7 true class 2\n",
      "test 62 prediction 3 true class 3\n",
      "test 63 prediction 7 true class 7\n",
      "test 64 prediction 1 true class 1\n",
      "test 65 prediction 1 true class 1\n",
      "test 66 prediction 0 true class 0\n",
      "test 67 prediction 6 true class 6\n",
      "test 68 prediction 8 true class 8\n",
      "test 69 prediction 7 true class 7\n",
      "test 70 prediction 8 true class 8\n",
      "test 71 prediction 3 true class 3\n",
      "test 72 prediction 7 true class 7\n",
      "test 73 prediction 9 true class 9\n",
      "test 74 prediction 3 true class 3\n",
      "test 75 prediction 2 true class 2\n",
      "test 76 prediction 7 true class 7\n",
      "test 77 prediction 1 true class 1\n",
      "test 78 prediction 1 true class 2\n",
      "test 79 prediction 7 true class 7\n",
      "test 80 prediction 4 true class 4\n",
      "test 81 prediction 8 true class 8\n",
      "test 82 prediction 1 true class 1\n",
      "test 83 prediction 5 true class 5\n",
      "test 84 prediction 1 true class 1\n",
      "test 85 prediction 5 true class 5\n",
      "test 86 prediction 1 true class 1\n",
      "test 87 prediction 7 true class 7\n",
      "test 88 prediction 4 true class 4\n",
      "test 89 prediction 7 true class 2\n",
      "test 90 prediction 3 true class 3\n",
      "test 91 prediction 1 true class 1\n",
      "test 92 prediction 5 true class 5\n",
      "test 93 prediction 9 true class 9\n",
      "test 94 prediction 7 true class 7\n",
      "test 95 prediction 4 true class 4\n",
      "test 96 prediction 1 true class 1\n",
      "test 97 prediction 6 true class 6\n",
      "test 98 prediction 9 true class 9\n",
      "test 99 prediction 9 true class 9\n",
      "test 100 prediction 5 true class 5\n",
      "test 101 prediction 2 true class 2\n",
      "test 102 prediction 5 true class 5\n",
      "test 103 prediction 0 true class 0\n",
      "test 104 prediction 3 true class 5\n",
      "test 105 prediction 9 true class 9\n",
      "test 106 prediction 6 true class 6\n",
      "test 107 prediction 1 true class 1\n",
      "test 108 prediction 2 true class 2\n",
      "test 109 prediction 6 true class 6\n",
      "test 110 prediction 9 true class 9\n",
      "test 111 prediction 0 true class 8\n",
      "test 112 prediction 7 true class 7\n",
      "test 113 prediction 3 true class 3\n",
      "test 114 prediction 6 true class 6\n",
      "test 115 prediction 7 true class 7\n",
      "test 116 prediction 5 true class 5\n",
      "test 117 prediction 2 true class 2\n",
      "test 118 prediction 5 true class 5\n",
      "test 119 prediction 1 true class 9\n",
      "test 120 prediction 4 true class 4\n",
      "test 121 prediction 4 true class 4\n",
      "test 122 prediction 5 true class 5\n",
      "test 123 prediction 0 true class 0\n",
      "test 124 prediction 1 true class 1\n",
      "test 125 prediction 9 true class 9\n",
      "test 126 prediction 3 true class 3\n",
      "test 127 prediction 7 true class 7\n",
      "test 128 prediction 6 true class 6\n",
      "test 129 prediction 0 true class 0\n",
      "test 130 prediction 7 true class 7\n",
      "test 131 prediction 6 true class 6\n",
      "test 132 prediction 5 true class 5\n",
      "test 133 prediction 4 true class 4\n",
      "test 134 prediction 1 true class 1\n",
      "test 135 prediction 1 true class 1\n",
      "test 136 prediction 0 true class 0\n",
      "test 137 prediction 4 true class 4\n",
      "test 138 prediction 2 true class 2\n",
      "test 139 prediction 3 true class 3\n",
      "test 140 prediction 8 true class 8\n",
      "test 141 prediction 7 true class 7\n",
      "test 142 prediction 9 true class 9\n",
      "test 143 prediction 4 true class 4\n",
      "test 144 prediction 4 true class 4\n",
      "test 145 prediction 5 true class 5\n",
      "test 146 prediction 6 true class 6\n",
      "test 147 prediction 3 true class 3\n",
      "test 148 prediction 4 true class 4\n",
      "test 149 prediction 3 true class 3\n",
      "test 150 prediction 9 true class 9\n",
      "test 151 prediction 8 true class 8\n",
      "test 152 prediction 0 true class 0\n",
      "test 153 prediction 9 true class 9\n",
      "test 154 prediction 4 true class 4\n",
      "test 155 prediction 4 true class 4\n",
      "test 156 prediction 0 true class 0\n",
      "test 157 prediction 0 true class 0\n",
      "test 158 prediction 0 true class 0\n",
      "test 159 prediction 2 true class 2\n",
      "test 160 prediction 7 true class 7\n",
      "test 161 prediction 3 true class 3\n",
      "test 162 prediction 7 true class 7\n",
      "test 163 prediction 6 true class 6\n",
      "test 164 prediction 5 true class 8\n",
      "test 165 prediction 8 true class 8\n",
      "test 166 prediction 7 true class 2\n",
      "test 167 prediction 8 true class 8\n",
      "test 168 prediction 6 true class 6\n",
      "test 169 prediction 5 true class 5\n",
      "test 170 prediction 6 true class 6\n",
      "test 171 prediction 1 true class 1\n",
      "test 172 prediction 5 true class 5\n",
      "test 173 prediction 5 true class 5\n",
      "test 174 prediction 4 true class 4\n",
      "test 175 prediction 0 true class 0\n",
      "test 176 prediction 6 true class 6\n",
      "test 177 prediction 6 true class 6\n",
      "test 178 prediction 2 true class 2\n",
      "test 179 prediction 6 true class 6\n",
      "test 180 prediction 0 true class 0\n",
      "test 181 prediction 5 true class 5\n",
      "test 182 prediction 8 true class 8\n",
      "test 183 prediction 1 true class 1\n",
      "test 184 prediction 6 true class 6\n",
      "test 185 prediction 6 true class 6\n",
      "test 186 prediction 2 true class 2\n",
      "test 187 prediction 7 true class 7\n",
      "test 188 prediction 5 true class 5\n",
      "test 189 prediction 3 true class 3\n",
      "test 190 prediction 3 true class 3\n",
      "test 191 prediction 1 true class 2\n",
      "test 192 prediction 9 true class 9\n",
      "test 193 prediction 8 true class 8\n",
      "test 194 prediction 6 true class 6\n",
      "test 195 prediction 0 true class 0\n",
      "test 196 prediction 6 true class 6\n",
      "test 197 prediction 0 true class 0\n",
      "test 198 prediction 1 true class 1\n",
      "test 199 prediction 8 true class 3\n",
      "Done!\n",
      "Accuracy 0.9350000000000007\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #loop over test data\n",
    "    for i in range(len(X_test)):\n",
    "        #get nearest neighbor\n",
    "        nn_index = sess.run(pred, feed_dict={x_train:X_train, x_test : X_test[i, :]})\n",
    "        #get nearest class label and compare it with its true label\n",
    "        print('test', i, 'prediction', np.argmax(Y_train[nn_index]),\n",
    "                                                'true class', np.argmax(Y_test[i]))\n",
    "        #calculate accuracy\n",
    "        if np.argmax(Y_train[nn_index]) == np.argmax(Y_test[i]):\n",
    "            accuracy += 1. / len(X_test)\n",
    "    print('Done!')\n",
    "    print('Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = mnist.train.images\n",
    "valid_X = mnist.validation.images\n",
    "test_X = mnist.test.images\n",
    "train_Y = mnist.train.labels\n",
    "valid_Y = mnist.validation.labels\n",
    "test_Y = mnist.test.labels\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_X)\n",
    "    tf_test_dataset = tf.constant(test_X)\n",
    "    \n",
    "    #variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #training computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    #predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) +biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 274.133240\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 20.3%\n",
      "Minibatch loss at step 50: 13.896990\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 100: 10.199195\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 150: 9.097733\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 200: 6.576060\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 250: 2.354981\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 300: 6.327595\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 350: 10.324965\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 400: 6.174293\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 450: 6.058807\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Test accuracy: 92.0%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "num_steps = 500\n",
    "##3001*128 总共的训练数据   ## 之前是801*10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        ##分母减去batch_size是防止当batch_size不能被训练数据量整除时，offset:(offset + batch_size)超出数组界限\n",
    "        offset = (step * batch_size) % (train_Y.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_X[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_Y[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_Y))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
