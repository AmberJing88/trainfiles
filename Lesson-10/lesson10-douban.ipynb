{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson-10 Movie comments classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. prepare the raw data\n",
    "2. sentences vectorization\n",
    "3. classify comments with Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/student/project/project-01/nlp_bots/hw-xujing/movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/nlp_bots/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(file_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261497, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty comments\n",
    "stars = list(data['star'])\n",
    "comments = list(data['comment'])\n",
    "index = list(data['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stars)):\n",
    "    if not isinstance(stars[i], str):\n",
    "        stars[i] = str(stars[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22047 1\n"
     ]
    }
   ],
   "source": [
    "remove1 = [_i for _i, com in enumerate(comments) if (not isinstance(com, str))\n",
    "          or com == '' or len(com) <= 4]\n",
    "remove2 = [_i for _i, star in enumerate(stars) if star not in ['1', '2', '3', '4', '5']]\n",
    "print(len(remove1), len(remove2))\n",
    "removes = remove1 + remove2\n",
    "removes = set(removes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_stars = [star for _i, star in enumerate(stars) if _i not in removes]\n",
    "movie_comments = [comm for _i, comm in enumerate(comments) if _i not in removes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239449, 239449)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_stars), len(movie_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditional Chinese convert to simplified Chinese\n",
    "from hanziconv import HanziConv\n",
    "def tra_to_sim(line):\n",
    "    trans = HanziConv.toSimplified(line)\n",
    "    return trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_comments = [tra_to_sim(comm) for comm in movie_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove not Chinese comments\n",
    "def is_chinese(string):\n",
    "    length = 0\n",
    "    for char in string:\n",
    "        if char >= '\\u4e00' and char <= '\\u9fff':\n",
    "            length += 1\n",
    "    if length > len(string) * 0.3:\n",
    "        return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.68 µs\n",
      "You got a dream, you gotta protect it. Don't let your dreams be dreams.\n",
      "You have a dream, you got to protect it.\n",
      "This is part of my life……\n",
      "This is part of my life……\n",
      "http://www.douban.com/review/1279547/\n",
      "2008.2.3\n",
      "Smith毁容鸟...\n",
      "2008.2.3\n",
      "Smith毁容鸟...\n",
      "http://v.youku.com/v_show/id_XODY3ODI3Ng==.html\n",
      "真尴尬啊。大结局居然是十年后问了声“my name is hanmeimei whats your name？”\n",
      "真尴尬啊。大结局居然是十年后问了声“my name is hanmeimei whats your name？”\n",
      "真尴尬啊。大结局居然是十年后问了声“my name is hanmeimei whats your name？”\n",
      "我只认识sandy and sue，再见\n",
      "ヽ(´o｀；\n",
      "ヽ(´o｀；\n",
      "ヽ(´o｀；\n",
      "ヽ(´o｀；\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for i in range(1000):\n",
    "    if not is_chinese(movie_comments[i]):\n",
    "        print(movie_comments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "removes = [i for i, com in enumerate(movie_comments) if not is_chinese(com)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9120"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(removes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stars = [star for i, star in enumerate(movie_stars) if i not in removes]\n",
    "clean_comments = [comm for i, comm in enumerate(movie_comments) if i not in removes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230329, 230329)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_stars), len(clean_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.classify different type according with stars\n",
    "2-classifier, 1-2 stars together, 3-5 stars together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1': 21370, '2': 25038, '4': 74220, '5': 51425, '3': 58276})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars_dict = Counter(clean_stars)\n",
    "stars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data: class pairs\n",
    "def get_classes(dataset, labels):\n",
    "    datas = defaultdict(list)\n",
    "    for i in range(len(labels)):\n",
    "        datas[labels[i]].append(dataset[i])\n",
    "    return datas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_dict = get_classes(clean_comments, clean_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1', '2', '4', '5', '3'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_label_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two-types classification as pos_comments for 4 to 5 stars, neg_comments for 1 to 2 stars, ignore 3-stars for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = defaultdict(list)\n",
    "data_dict[0] = class_label_dict['1'] + class_label_dict['2']\n",
    "data_dict[1] = class_label_dict['4'] + class_label_dict['5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46408, 125645)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict[0]), len(data_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# choose balanced negative and positive samples \n",
    "sample_numbers = 45000\n",
    "def get_balanced_data(data, sample_numbers):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for key in data:\n",
    "        if len(data[key]) < sample_numbers:\n",
    "            dataset += data[key]\n",
    "            labels += [key] * len(data[key])\n",
    "        else:\n",
    "            dataset += random.sample(data[key], sample_numbers)\n",
    "            labels += [key] * sample_numbers\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = get_balanced_data(data_dict, sample_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 90000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打擂台叫五毒教上啊 0\n"
     ]
    }
   ],
   "source": [
    "print(dataset[250], labels[250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF documents vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[\\w, 。；：！（）《》、“”‘’……]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.864 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#with open('/home/student/project/project-01/nlp_bots/hw-xujing/comment.txt', 'w', encoding='utf-8') as f:\n",
    "    comments_cut = []\n",
    "    for da in dataset:\n",
    "        if not da.strip(): continue\n",
    "        comment = ''.join(pattern.findall(da))\n",
    "        comment = ' '.join(jieba.cut(comment))\n",
    "        if not comment: continue\n",
    "        comments_cut.append(comment)\n",
    "        #f.write(comment+'/n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "sentences = LineSentence('/home/student/project/project-01/nlp_bots/hw-xujing/comment.txt')\n",
    "model = Word2Vec(sentences=sentences, size=300, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/student/project/project-01/nlp_bots/hw-xujing/douban.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('/home/student/project/project-01/nlp_bots/hw-xujing/douban.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['好看'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50527"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#training sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1500)\n",
    "X = tfidf.fit_transform(comments_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 1500)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, labels, test_size=0.2)\n",
    "valid_dataset, test_dataset, valid_labels, test_labels = train_test_split(test_dataset, test_labels, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72000, 1500) (9000, 1500) (9000, 1500)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape, valid_dataset.shape, test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/nlp_bots/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7512\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_dataset, train_labels)\n",
    "y_hat = clf.predict(valid_dataset)\n",
    "acc = format(accuracy_score(valid_labels, y_hat), '.4f')\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save classified data\n",
    "# here save in metrics format for tfidf vectors\n",
    "from six.moves import cPickle as pickle\n",
    "pickle_file = '/home/student/project/project-01/nlp_bots/hw-xujing/douban-tfidf'\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        save = {\n",
    "            'train_dataset' : train_dataset,\n",
    "            'train_labels' : train_labels,\n",
    "            'valid_dataset' : valid_dataset,\n",
    "            'valid_labels' : valid_labels, \n",
    "            'test_dataset' : test_dataset,\n",
    "            'test_labels' : test_labels\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('unable to save data to', pickle_file, ':', e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set (72000, 1500) (72000,)\n",
      "valid set (9000, 1500) (9000,)\n",
      "test set (9000, 1500) (9000, 1500)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pickle_file = '/home/student/project/project-01/nlp_bots/hw-xujing/douban-tfidf'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del(save)\n",
    "    print('train set', train_dataset.shape, train_labels.shape)\n",
    "    print('valid set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('test set', test_dataset.shape, test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set (72000, 1500) (72000, 2)\n",
      "valid set (9000, 1500) (9000, 2)\n",
      "test set (9000, 1500) (9000, 2)\n"
     ]
    }
   ],
   "source": [
    "#unify data classes, make labels to one-hot \n",
    "import numpy as np\n",
    "num_labels = 2\n",
    "def reformat(dataset, labels):\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    datasets = dataset.astype(np.float32)\n",
    "    datasets = datasets.toarray()\n",
    "    return datasets, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('train set', train_dataset.shape, train_labels.shape)\n",
    "print('valid set', valid_dataset.shape, valid_labels.shape)\n",
    "print('test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two layer nn\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 128\n",
    "beta_regul = 1e-3\n",
    "feature_size = 1500\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, feature_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step =tf.Variable(0)\n",
    "    \n",
    "    #variables\n",
    "    # layer 1: stddev is the pointed derived data standard mean \n",
    "    weights1 = tf.Variable(tf.truncated_normal([feature_size, num_hidden_nodes1], stddev=np.sqrt(2.0 / (feature_size))))\n",
    "    bias1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    #layer 2: input num_hidden_nodes1, \n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], \n",
    "                                               stddev=np.sqrt(2.0 / (num_hidden_nodes1))))\n",
    "    bias2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    #last layer, input num_hidden_nodes2, current nodes= num_labels\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_labels],\n",
    "                                              stddev=np.sqrt(2.0 / (num_hidden_nodes2))))\n",
    "    bias3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #training computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + bias1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) +bias2)\n",
    "    logits = tf.matmul(lay2_train, weights3) +bias3\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    labels=tf_train_labels, logits=logits))\n",
    "    L2 = beta_regul * (tf.nn.l2_loss(weights1) +tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "    loss = tf.reduce_mean(loss + L2)\n",
    "    \n",
    "    #optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    #predictions for train valid test\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + bias1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) +bias2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + bias3)\n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + bias1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + bias2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "minibatch loss as step 0: 1.585643\n",
      "minibatch accuracy: 46.9%\n",
      "Validation accuracy: 50.9%\n",
      "minibatch loss as step 200: 1.537333\n",
      "minibatch accuracy: 68.8%\n",
      "Validation accuracy: 57.3%\n",
      "minibatch loss as step 400: 1.485980\n",
      "minibatch accuracy: 63.3%\n",
      "Validation accuracy: 65.8%\n",
      "minibatch loss as step 600: 1.397760\n",
      "minibatch accuracy: 68.0%\n",
      "Validation accuracy: 73.3%\n",
      "minibatch loss as step 800: 1.308116\n",
      "minibatch accuracy: 70.3%\n",
      "Validation accuracy: 73.4%\n",
      "minibatch loss as step 1000: 1.263986\n",
      "minibatch accuracy: 69.5%\n",
      "Validation accuracy: 75.5%\n",
      "minibatch loss as step 1200: 1.259528\n",
      "minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.8%\n",
      "minibatch loss as step 1400: 1.176658\n",
      "minibatch accuracy: 80.5%\n",
      "Validation accuracy: 75.6%\n",
      "minibatch loss as step 1600: 1.215387\n",
      "minibatch accuracy: 72.7%\n",
      "Validation accuracy: 72.3%\n",
      "minibatch loss as step 1800: 1.160054\n",
      "minibatch accuracy: 76.6%\n",
      "Validation accuracy: 73.7%\n",
      "minibatch loss as step 2000: 1.169119\n",
      "minibatch accuracy: 75.8%\n",
      "Validation accuracy: 74.7%\n",
      "minibatch loss as step 2200: 1.120767\n",
      "minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.7%\n",
      "minibatch loss as step 2400: 1.133723\n",
      "minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.9%\n",
      "minibatch loss as step 2600: 1.063840\n",
      "minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.7%\n",
      "minibatch loss as step 2800: 1.181902\n",
      "minibatch accuracy: 71.9%\n",
      "Validation accuracy: 74.4%\n",
      "minibatch loss as step 3000: 1.137821\n",
      "minibatch accuracy: 70.3%\n",
      "Validation accuracy: 77.1%\n",
      "minibatch loss as step 3200: 1.063761\n",
      "minibatch accuracy: 73.4%\n",
      "Validation accuracy: 76.9%\n",
      "minibatch loss as step 3400: 1.023725\n",
      "minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.9%\n",
      "minibatch loss as step 3600: 1.048240\n",
      "minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.2%\n",
      "Test accuracy: 76.4%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "num_steps = 3601\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # pick an offset within the training data, which has been randomized.\n",
    "        #分母减去batch_size是防止当batch_size不能被训练数据量整除时，offset:(offset + batch_size)超出数组界限\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        #generate a mini batch\n",
    "        batch_data = train_dataset[offset:(offset +batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        #prepare a dictionary telling the session where to feed teh minibatch.\n",
    "        #the key of the dictionary is the placeholder to feed teh minibatch.\n",
    "        #and teh value is teh numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 200 == 0):\n",
    "            print('minibatch loss as step %d: %f' % (step, l))\n",
    "            print('minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF准确率76.4%，需要进一步处理数据，或Learning rate试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
